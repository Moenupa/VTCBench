[project]
name = "llm-serving"
version = "0.1.0"
description = "A simple demonstration of serving large language models using vLLM."
readme = "README.md"
requires-python = ">=3.12,<3.13"
dependencies = [
    "vllm==0.11.0",
    "torch>=2.8.0",
    "torchvision",
    "flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl"
]

[project.optional-dependencies]
flashinfer = [
    "flashinfer-python",
    "flashinfer-cubin",
]

[tool.uv.sources]
torch = [
	{ index = "pytorch-cu128", marker = "sys_platform == 'linux' or sys_platform == 'win32'" },
]
torchvision = [
	{ index = "pytorch-cu128", marker = "sys_platform == 'linux' or sys_platform == 'win32'" },
]

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.uv]
no-build-isolation-package = ["flash-attn"]
